{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DlAfI8mCWAf3"
   },
   "source": [
    "# Transfer learning for NLP\n",
    "## ALTEGRAD - Lab session 3\n",
    "#### Moussa Kamal Eddine, Hadi Abdine (Dascim LIX)\n",
    "##### November 2020"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "IqukuIe0Rb_c"
   },
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5FF6fjkqgN39"
   },
   "source": [
    "### The Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "p0cj9WkSFQwl"
   },
   "outputs": [],
   "source": [
    "class TransformerModel(nn.Module):\n",
    "    def __init__(self, ntoken, nhead, nhid, nlayers, dropout=0.5):\n",
    "        super(TransformerModel, self).__init__()\n",
    "        '''\n",
    "        ntokens: the size of vocabulary\n",
    "        nhid: the hidden dimension of the model.\n",
    "        We assume that embedding_dim = nhid\n",
    "        nlayers: the number of nn.TransformerEncoderLayer in nn.TransformerEncoder\n",
    "        nhead: the number of heads in the multiheadattention models\n",
    "        dropout: the dropout value\n",
    "         '''\n",
    "        self.model_type = \"Transformer\"\n",
    "        self.encoder = nn.Embedding(ntokens,nhid) # fill me, nhid = the dim_embed\n",
    "        self.pos_encoder = PositionalEncoding(nhid,dropout) #fill me, the PositionalEncoding class is implemented in the next cell\n",
    "        encoder_layers = nn.TransformerEncoderLayer(nhid,nhead,nhid,dropout) #fill me we assume nhid = d_model = dim_feedforward\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layers,nlayers) #fill me\n",
    "        self.nhid = nhid\n",
    "        self.init_weights()\n",
    "\n",
    "    def generate_square_subsequent_mask(self, sz):\n",
    "        mask = (torch.triu(torch.ones(sz, sz)) == 1).transpose(0, 1)\n",
    "        mask = (\n",
    "            mask.float()\n",
    "            .masked_fill(mask == 0, float(\"-inf\"))\n",
    "            .masked_fill(mask == 1, float(0.0))\n",
    "        )\n",
    "        return mask\n",
    "\n",
    "    def init_weights(self):\n",
    "        initrange = 0.1\n",
    "        self.encoder.weight.data.uniform_(-initrange, initrange)\n",
    "\n",
    "    def forward(self, src, src_mask):\n",
    "        src = self.encoder(src) * math.sqrt(self.nhid) \n",
    "        src = self.pos_encoder(src) #fill me\n",
    "        output = self.transformer_encoder(src, src_mask)\n",
    "        return output\n",
    "\n",
    "\n",
    "class ClassificationHead(nn.Module):\n",
    "    def __init__(self, nhid, nclasses):\n",
    "        super(ClassificationHead, self).__init__()\n",
    "        self.decoder = nn.Linear(nhid,nclasses)\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        initrange = 0.1\n",
    "        self.decoder.bias.data.zero_()\n",
    "        self.decoder.weight.data.uniform_(-initrange, initrange)\n",
    "\n",
    "    def forward(self, src):\n",
    "        output = self.decoder(src)\n",
    "        return output\n",
    "    \n",
    "class Model(nn.Module):\n",
    "    def __init__(self, ntoken, nhead, nhid, nlayers, nclasses, dropout=0.5):\n",
    "        super(Model, self).__init__()\n",
    "        self.base = TransformerModel(ntoken, nhead, nhid, nlayers, dropout)#fill me\n",
    "        self.classifier = ClassificationHead(nhid, nclasses)#fill me \n",
    "\n",
    "    def forward(self, src, src_mask):\n",
    "        # base model\n",
    "        x = self.base.forward(src,src_mask) #fill me\n",
    "        # classifier model\n",
    "        output = self.classifier.forward(x) #fill me\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "kt2QQohaFZry"
   },
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, nhid, dropout=0.1, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        pe = torch.zeros(max_len, nhid)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(\n",
    "            torch.arange(0, nhid, 2).float() * (-math.log(10000.0) / nhid)\n",
    "        )\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
    "        self.register_buffer(\"pe\", pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[: x.size(0), :]\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "pYr1yMnd7mTZ"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SfEYHJx2JW6l"
   },
   "source": [
    "Let's verify if our model works, by applying one inference step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rhb2gkUhJMR0",
    "outputId": "7991537b-b393-4a11-f264-4eb307a1488f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 6, 100])\n"
     ]
    }
   ],
   "source": [
    "ntokens = 100 #fill me # the size of vocabulary\n",
    "nhid = 200  # hidden dimension\n",
    "nlayers = 4  # the number of nn.TransformerEncoderLayer in nn.TransformerEncoder\n",
    "nhead = 2  # the number of heads in the multiheadattention models\n",
    "dropout = 0  # the dropout value\n",
    "\n",
    "model = Model(ntokens, nhead, nhid, nlayers, ntokens, dropout).to(device)\n",
    "dummy_input = torch.tensor([[2, 6, 2, 5, 43, 21]]).to(device)\n",
    "src_mask = model.base.generate_square_subsequent_mask(1).to(device)\n",
    "out = model.forward(dummy_input, src_mask)\n",
    "\n",
    "print(out.shape) # is it the right shape?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5ur73Lfz99Ii",
    "outputId": "00eadfa3-4eb9-41b5-f18e-323236f58012"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "base.encoder.weight : torch.Size([100, 200])\n",
      "base.transformer_encoder.layers.0.self_attn.in_proj_weight : torch.Size([600, 200])\n",
      "base.transformer_encoder.layers.0.self_attn.in_proj_bias : torch.Size([600])\n",
      "base.transformer_encoder.layers.0.self_attn.out_proj.weight : torch.Size([200, 200])\n",
      "base.transformer_encoder.layers.0.self_attn.out_proj.bias : torch.Size([200])\n",
      "base.transformer_encoder.layers.0.linear1.weight : torch.Size([200, 200])\n",
      "base.transformer_encoder.layers.0.linear1.bias : torch.Size([200])\n",
      "base.transformer_encoder.layers.0.linear2.weight : torch.Size([200, 200])\n",
      "base.transformer_encoder.layers.0.linear2.bias : torch.Size([200])\n",
      "base.transformer_encoder.layers.0.norm1.weight : torch.Size([200])\n",
      "base.transformer_encoder.layers.0.norm1.bias : torch.Size([200])\n",
      "base.transformer_encoder.layers.0.norm2.weight : torch.Size([200])\n",
      "base.transformer_encoder.layers.0.norm2.bias : torch.Size([200])\n",
      "base.transformer_encoder.layers.1.self_attn.in_proj_weight : torch.Size([600, 200])\n",
      "base.transformer_encoder.layers.1.self_attn.in_proj_bias : torch.Size([600])\n",
      "base.transformer_encoder.layers.1.self_attn.out_proj.weight : torch.Size([200, 200])\n",
      "base.transformer_encoder.layers.1.self_attn.out_proj.bias : torch.Size([200])\n",
      "base.transformer_encoder.layers.1.linear1.weight : torch.Size([200, 200])\n",
      "base.transformer_encoder.layers.1.linear1.bias : torch.Size([200])\n",
      "base.transformer_encoder.layers.1.linear2.weight : torch.Size([200, 200])\n",
      "base.transformer_encoder.layers.1.linear2.bias : torch.Size([200])\n",
      "base.transformer_encoder.layers.1.norm1.weight : torch.Size([200])\n",
      "base.transformer_encoder.layers.1.norm1.bias : torch.Size([200])\n",
      "base.transformer_encoder.layers.1.norm2.weight : torch.Size([200])\n",
      "base.transformer_encoder.layers.1.norm2.bias : torch.Size([200])\n",
      "base.transformer_encoder.layers.2.self_attn.in_proj_weight : torch.Size([600, 200])\n",
      "base.transformer_encoder.layers.2.self_attn.in_proj_bias : torch.Size([600])\n",
      "base.transformer_encoder.layers.2.self_attn.out_proj.weight : torch.Size([200, 200])\n",
      "base.transformer_encoder.layers.2.self_attn.out_proj.bias : torch.Size([200])\n",
      "base.transformer_encoder.layers.2.linear1.weight : torch.Size([200, 200])\n",
      "base.transformer_encoder.layers.2.linear1.bias : torch.Size([200])\n",
      "base.transformer_encoder.layers.2.linear2.weight : torch.Size([200, 200])\n",
      "base.transformer_encoder.layers.2.linear2.bias : torch.Size([200])\n",
      "base.transformer_encoder.layers.2.norm1.weight : torch.Size([200])\n",
      "base.transformer_encoder.layers.2.norm1.bias : torch.Size([200])\n",
      "base.transformer_encoder.layers.2.norm2.weight : torch.Size([200])\n",
      "base.transformer_encoder.layers.2.norm2.bias : torch.Size([200])\n",
      "base.transformer_encoder.layers.3.self_attn.in_proj_weight : torch.Size([600, 200])\n",
      "base.transformer_encoder.layers.3.self_attn.in_proj_bias : torch.Size([600])\n",
      "base.transformer_encoder.layers.3.self_attn.out_proj.weight : torch.Size([200, 200])\n",
      "base.transformer_encoder.layers.3.self_attn.out_proj.bias : torch.Size([200])\n",
      "base.transformer_encoder.layers.3.linear1.weight : torch.Size([200, 200])\n",
      "base.transformer_encoder.layers.3.linear1.bias : torch.Size([200])\n",
      "base.transformer_encoder.layers.3.linear2.weight : torch.Size([200, 200])\n",
      "base.transformer_encoder.layers.3.linear2.bias : torch.Size([200])\n",
      "base.transformer_encoder.layers.3.norm1.weight : torch.Size([200])\n",
      "base.transformer_encoder.layers.3.norm1.bias : torch.Size([200])\n",
      "base.transformer_encoder.layers.3.norm2.weight : torch.Size([200])\n",
      "base.transformer_encoder.layers.3.norm2.bias : torch.Size([200])\n",
      "classifier.decoder.weight : torch.Size([100, 200])\n",
      "classifier.decoder.bias : torch.Size([100])\n"
     ]
    }
   ],
   "source": [
    "for name, parameters in model.named_parameters():\n",
    "  print(name,\":\", parameters.size()) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i74NN897Fcit"
   },
   "source": [
    "## Vocabulary and Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5qjd26ghWuff",
    "outputId": "336c3c12-db91-44a4-f447-df91995a2112"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2020-12-08 19:21:24--  https://raw.githubusercontent.com/moussaKam/transfer_learning_transformers/main/dict.txt\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 577587 (564K) [text/plain]\n",
      "Saving to: ‘dict.txt’\n",
      "\n",
      "dict.txt            100%[===================>] 564.05K  --.-KB/s    in 0.03s   \n",
      "\n",
      "2020-12-08 19:21:24 (18.6 MB/s) - ‘dict.txt’ saved [577587/577587]\n",
      "\n",
      "▁d 1\n",
      "es 1\n",
      "▁l 1\n",
      "en 1\n",
      "on 1\n"
     ]
    }
   ],
   "source": [
    "!wget https://raw.githubusercontent.com/moussaKam/transfer_learning_transformers/main/dict.txt\n",
    "!head -5 dict.txt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vFdH_-JeFbGA",
    "outputId": "0efe3fc6-327f-4301-bc4c-2202c5ef89fa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "▁trop\n",
      "1111\n"
     ]
    }
   ],
   "source": [
    "\n",
    "path_vocab = \"dict.txt\"\n",
    "token2ind = {\"<sos>\": 0, \"<pad>\": 1, \"<eos>\": 2, \"<oov>\": 3} # the 4 first indices are reserved to special tokens\n",
    "with open(path_vocab, \"r\") as f:\n",
    "    for idx, line in enumerate(f):\n",
    "        word = line.split()[0].strip()\n",
    "        token2ind[word] = idx + 4  #fill me\n",
    "\n",
    "\n",
    "ind2token = list(token2ind.keys()) #fill me\n",
    "\n",
    "\n",
    "print(ind2token[1111])\n",
    "print(token2ind[\"▁trop\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XOExGODajN8p"
   },
   "source": [
    "### Data Loader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "Y0jN-Ar9i5Q1"
   },
   "outputs": [],
   "source": [
    "import numpy\n",
    "import torch\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "\n",
    "class Dataset(Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        path_documents,\n",
    "        path_labels=None,\n",
    "        token2ind={},\n",
    "        max_len=512,\n",
    "        task=\"language_modeling\",\n",
    "    ):\n",
    "        self.task = task\n",
    "        self.max_len = max_len\n",
    "        self.token2ind = token2ind\n",
    "        self.documents = []\n",
    "        self.labels = []\n",
    "        with open(path_documents, \"r\") as f1:\n",
    "            for line in f1:\n",
    "                self.documents.append(line.strip())\n",
    "        if task == \"classification\":\n",
    "            with open(path_labels, \"r\") as f1:\n",
    "                for line in f1:\n",
    "                    self.labels.append(int(line.strip()))\n",
    "            assert len(self.labels) == len(self.documents)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.documents)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        sequence = self.documents[index].split()\n",
    "        if len(sequence) > self.max_len - 1:\n",
    "            sequence = sequence[: self.max_len - 1]\n",
    "        source_sequence = [self.token2ind[\"<sos>\"]] + [\n",
    "            self.token2ind[word] if word in self.token2ind else self.token2ind[\"<oov>\"]\n",
    "            for word in sequence[: self.max_len]\n",
    "        ]\n",
    "        if self.task == \"language_modeling\":\n",
    "            target = source_sequence[1:]\n",
    "            target.append(self.token2ind[\"<eos>\"])\n",
    "        elif self.task == \"classification\":\n",
    "            target = [self.labels[index]]\n",
    "        sample = {\n",
    "            \"source_sequence\": torch.tensor(source_sequence),\n",
    "            \"target\": torch.tensor(target),\n",
    "        }\n",
    "        return sample\n",
    "\n",
    "\n",
    "def MyCollator(batch):\n",
    "    source_sequences = pad_sequence(\n",
    "        #we use padding to match the length of the sequences in the same batch\n",
    "        [sample[\"source_sequence\"] for sample in batch], padding_value=token2ind[\"<pad>\"]\n",
    "    )\n",
    "    target = pad_sequence(\n",
    "        [sample[\"target\"] for sample in batch], padding_value=token2ind[\"<pad>\"]\n",
    "    )\n",
    "    return source_sequences, target.reshape(-1)\n",
    "\n",
    "\n",
    "def get_loader(\n",
    "    path_documents,\n",
    "    path_labels=None,\n",
    "    token2ind={},\n",
    "    max_len=512,\n",
    "    batch_size=32,\n",
    "    task=\"language_modeling\",\n",
    "):\n",
    "    dataset = Dataset(\n",
    "        path_documents,\n",
    "        path_labels=path_labels,\n",
    "        token2ind=token2ind,\n",
    "        max_len=512,\n",
    "        task=task,\n",
    "    )\n",
    "    data_loader = DataLoader(\n",
    "        dataset=dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        collate_fn=MyCollator,\n",
    "        pin_memory=True,\n",
    "        drop_last=True,\n",
    "    )\n",
    "    return data_loader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uTns4lHrjUTa"
   },
   "source": [
    "## The Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "4_jwosiLjRsS"
   },
   "outputs": [],
   "source": [
    "def train(\n",
    "    path_data_train,\n",
    "    path_labels_train=None,\n",
    "    path_data_valid=None,\n",
    "    save_interval=-1,\n",
    "    log_interval=5,\n",
    "    task=\"language_modeling\",\n",
    "    batch_size=32,\n",
    "):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    ntokens = len(token2ind)\n",
    "    data_loader = get_loader(\n",
    "        path_data_train,\n",
    "        path_labels_train,\n",
    "        token2ind,\n",
    "        task=task,\n",
    "        batch_size=batch_size,\n",
    "    )\n",
    "    \n",
    "    losses = []\n",
    "    for idx, data in enumerate(data_loader): #step 1\n",
    "        optimizer.zero_grad()\n",
    "        src_mask = model.base.generate_square_subsequent_mask(data[0].size(0)).to(\n",
    "            device\n",
    "        )\n",
    "        input = data[0].to(device)\n",
    "        output = model(input, src_mask) #step 2\n",
    "        if task == 'classification':\n",
    "            #last vector only\n",
    "            output = output[-1]  #fill me \n",
    "        output = output.view(-1, output.shape[-1])\n",
    "        target = data[1] #fill me\n",
    "        target = target.to(device)\n",
    "        loss =  criterion(output,target)#fill me, Cross entropy check next cells\n",
    "        #fill me step 3\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.5) # prevent exploding gradient \n",
    "        #fill me step 4\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item() \n",
    "        if idx % log_interval == 0 and idx > 0:\n",
    "            cur_loss = total_loss / log_interval\n",
    "            print(\n",
    "                \"| epoch {:3d} | {:5d}/{:5d} steps | \"\n",
    "                \"loss {:5.5f} | ppl {:8.3f}\".format(\n",
    "                    epoch, idx, len(data_loader), cur_loss, math.exp(cur_loss),\n",
    "                )\n",
    "            )\n",
    "            losses.append(cur_loss)\n",
    "            total_loss = 0\n",
    "    return losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "pgf6BDB9jUr6"
   },
   "outputs": [],
   "source": [
    "ntokens = len(ind2token) #fill me # the size of vocabulary\n",
    "nhid = 200  # the dimension of the feedforward network model in nn.TransformerEncoder\n",
    "nlayers = 4  # the number of nn.TransformerEncoderLayer in nn.TransformerEncoder\n",
    "nhead = 2  # the number of heads in the multiheadattention models\n",
    "dropout = 0  # the dropout value\n",
    "\n",
    "nclasses = 2 # for classification task only\n",
    "\n",
    "model = Model(ntokens, nhead, nhid, nlayers, ntokens, dropout).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "u-OLy4KIkDwf"
   },
   "outputs": [],
   "source": [
    "# optimization paramerters\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=token2ind['<pad>'])\n",
    "lr = 0.0003  # learning rate\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Bwh3n9xZQy4e",
    "outputId": "cff1d9bf-2bc3-496f-b30b-df0bc6de8fab"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2020-12-08 19:21:25--  https://raw.githubusercontent.com/moussaKam/transfer_learning_transformers/main/pretraining_subset.txt\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 10146460 (9.7M) [text/plain]\n",
      "Saving to: ‘pretraining_subset.txt’\n",
      "\n",
      "pretraining_subset. 100%[===================>]   9.68M  53.3MB/s    in 0.2s    \n",
      "\n",
      "2020-12-08 19:21:25 (53.3 MB/s) - ‘pretraining_subset.txt’ saved [10146460/10146460]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://raw.githubusercontent.com/moussaKam/transfer_learning_transformers/main/pretraining_subset.txt\n",
    "path_data_train = \"pretraining_subset.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0m11g4ScjZaR",
    "outputId": "a7553237-f907-42ff-8f2d-87186cc095d1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   1 |   500/ 3125 steps | loss 7.30814 | ppl 1492.400\n",
      "| epoch   1 |  1000/ 3125 steps | loss 6.50192 | ppl  666.420\n",
      "| epoch   1 |  1500/ 3125 steps | loss 6.20296 | ppl  494.210\n",
      "| epoch   1 |  2000/ 3125 steps | loss 6.03552 | ppl  418.018\n",
      "| epoch   1 |  2500/ 3125 steps | loss 5.90807 | ppl  367.997\n",
      "| epoch   1 |  3000/ 3125 steps | loss 5.82216 | ppl  337.702\n",
      "| epoch   2 |   500/ 3125 steps | loss 5.51676 | ppl  248.828\n",
      "| epoch   2 |  1000/ 3125 steps | loss 5.47406 | ppl  238.427\n",
      "| epoch   2 |  1500/ 3125 steps | loss 5.44690 | ppl  232.037\n",
      "| epoch   2 |  2000/ 3125 steps | loss 5.42942 | ppl  228.016\n",
      "| epoch   2 |  2500/ 3125 steps | loss 5.39096 | ppl  219.413\n",
      "| epoch   2 |  3000/ 3125 steps | loss 5.35042 | ppl  210.696\n"
     ]
    }
   ],
   "source": [
    "#pretraining on a tiny subset\n",
    "log_interval = 500\n",
    "epochs = 2\n",
    "for epoch in range(1, epochs + 1): #5\n",
    "    train(\n",
    "        path_data_train,\n",
    "        save_interval=-1,\n",
    "        task=\"language_modeling\",\n",
    "        batch_size=16,\n",
    "        log_interval=log_interval,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MeOM1dOvkO4e"
   },
   "source": [
    "## Text Generation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-BcBC6FSkMH3",
    "outputId": "ccf834ba-bff7-4713-e144-24b1db01b56f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2020-12-08 19:27:13--  https://raw.githubusercontent.com/moussaKam/transfer_learning_transformers/main/pretrained_model_4layers.pt\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 88093955 (84M) [application/octet-stream]\n",
      "Saving to: ‘pretrained_model_4layers.pt’\n",
      "\n",
      "pretrained_model_4l 100%[===================>]  84.01M   167MB/s    in 0.5s    \n",
      "\n",
      "2020-12-08 19:27:15 (167 MB/s) - ‘pretrained_model_4layers.pt’ saved [88093955/88093955]\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 21,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!wget https://raw.githubusercontent.com/moussaKam/transfer_learning_transformers/main/pretrained_model_4layers.pt\n",
    "\n",
    "model = Model(ntokens, nhead, nhid, nlayers, ntokens).to(device)\n",
    "\n",
    "#load the checkpoint\n",
    "checkpoint = torch.load('pretrained_model_4layers.pt') \n",
    "#load state dict\n",
    "model.load_state_dict(checkpoint['model_state_dict']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tBRRVsWqlIoQ",
    "outputId": "3b27b90b-d72a-4911-fb40-58c01a33f074"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting sentencepiece\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e5/2d/6d4ca4bef9a67070fa1cac508606328329152b1df10bdf31fb6e4e727894/sentencepiece-0.1.94-cp36-cp36m-manylinux2014_x86_64.whl (1.1MB)\n",
      "\r",
      "\u001b[K     |▎                               | 10kB 21.7MB/s eta 0:00:01\r",
      "\u001b[K     |▋                               | 20kB 29.1MB/s eta 0:00:01\r",
      "\u001b[K     |▉                               | 30kB 15.2MB/s eta 0:00:01\r",
      "\u001b[K     |█▏                              | 40kB 10.8MB/s eta 0:00:01\r",
      "\u001b[K     |█▌                              | 51kB 7.6MB/s eta 0:00:01\r",
      "\u001b[K     |█▊                              | 61kB 8.2MB/s eta 0:00:01\r",
      "\u001b[K     |██                              | 71kB 8.5MB/s eta 0:00:01\r",
      "\u001b[K     |██▍                             | 81kB 8.6MB/s eta 0:00:01\r",
      "\u001b[K     |██▋                             | 92kB 8.5MB/s eta 0:00:01\r",
      "\u001b[K     |███                             | 102kB 7.5MB/s eta 0:00:01\r",
      "\u001b[K     |███▎                            | 112kB 7.5MB/s eta 0:00:01\r",
      "\u001b[K     |███▌                            | 122kB 7.5MB/s eta 0:00:01\r",
      "\u001b[K     |███▉                            | 133kB 7.5MB/s eta 0:00:01\r",
      "\u001b[K     |████▏                           | 143kB 7.5MB/s eta 0:00:01\r",
      "\u001b[K     |████▍                           | 153kB 7.5MB/s eta 0:00:01\r",
      "\u001b[K     |████▊                           | 163kB 7.5MB/s eta 0:00:01\r",
      "\u001b[K     |█████                           | 174kB 7.5MB/s eta 0:00:01\r",
      "\u001b[K     |█████▎                          | 184kB 7.5MB/s eta 0:00:01\r",
      "\u001b[K     |█████▋                          | 194kB 7.5MB/s eta 0:00:01\r",
      "\u001b[K     |█████▉                          | 204kB 7.5MB/s eta 0:00:01\r",
      "\u001b[K     |██████▏                         | 215kB 7.5MB/s eta 0:00:01\r",
      "\u001b[K     |██████▌                         | 225kB 7.5MB/s eta 0:00:01\r",
      "\u001b[K     |██████▊                         | 235kB 7.5MB/s eta 0:00:01\r",
      "\u001b[K     |███████                         | 245kB 7.5MB/s eta 0:00:01\r",
      "\u001b[K     |███████▍                        | 256kB 7.5MB/s eta 0:00:01\r",
      "\u001b[K     |███████▋                        | 266kB 7.5MB/s eta 0:00:01\r",
      "\u001b[K     |████████                        | 276kB 7.5MB/s eta 0:00:01\r",
      "\u001b[K     |████████▎                       | 286kB 7.5MB/s eta 0:00:01\r",
      "\u001b[K     |████████▌                       | 296kB 7.5MB/s eta 0:00:01\r",
      "\u001b[K     |████████▉                       | 307kB 7.5MB/s eta 0:00:01\r",
      "\u001b[K     |█████████                       | 317kB 7.5MB/s eta 0:00:01\r",
      "\u001b[K     |█████████▍                      | 327kB 7.5MB/s eta 0:00:01\r",
      "\u001b[K     |█████████▊                      | 337kB 7.5MB/s eta 0:00:01\r",
      "\u001b[K     |██████████                      | 348kB 7.5MB/s eta 0:00:01\r",
      "\u001b[K     |██████████▎                     | 358kB 7.5MB/s eta 0:00:01\r",
      "\u001b[K     |██████████▋                     | 368kB 7.5MB/s eta 0:00:01\r",
      "\u001b[K     |██████████▉                     | 378kB 7.5MB/s eta 0:00:01\r",
      "\u001b[K     |███████████▏                    | 389kB 7.5MB/s eta 0:00:01\r",
      "\u001b[K     |███████████▌                    | 399kB 7.5MB/s eta 0:00:01\r",
      "\u001b[K     |███████████▊                    | 409kB 7.5MB/s eta 0:00:01\r",
      "\u001b[K     |████████████                    | 419kB 7.5MB/s eta 0:00:01\r",
      "\u001b[K     |████████████▍                   | 430kB 7.5MB/s eta 0:00:01\r",
      "\u001b[K     |████████████▋                   | 440kB 7.5MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████                   | 450kB 7.5MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████▎                  | 460kB 7.5MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████▌                  | 471kB 7.5MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████▉                  | 481kB 7.5MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████                  | 491kB 7.5MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████▍                 | 501kB 7.5MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████▊                 | 512kB 7.5MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████                 | 522kB 7.5MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████▎                | 532kB 7.5MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████▋                | 542kB 7.5MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████▉                | 552kB 7.5MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████▏               | 563kB 7.5MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████▌               | 573kB 7.5MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████▊               | 583kB 7.5MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████               | 593kB 7.5MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████▍              | 604kB 7.5MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████▋              | 614kB 7.5MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████              | 624kB 7.5MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████▏             | 634kB 7.5MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████▌             | 645kB 7.5MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████▉             | 655kB 7.5MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████             | 665kB 7.5MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████▍            | 675kB 7.5MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████▊            | 686kB 7.5MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████            | 696kB 7.5MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████▎           | 706kB 7.5MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████▋           | 716kB 7.5MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████▉           | 727kB 7.5MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████▏          | 737kB 7.5MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████▌          | 747kB 7.5MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████▊          | 757kB 7.5MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████          | 768kB 7.5MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████▎         | 778kB 7.5MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████▋         | 788kB 7.5MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████         | 798kB 7.5MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████▏        | 808kB 7.5MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████▌        | 819kB 7.5MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████▉        | 829kB 7.5MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████        | 839kB 7.5MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████▍       | 849kB 7.5MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████▊       | 860kB 7.5MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████████       | 870kB 7.5MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████████▎      | 880kB 7.5MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████████▋      | 890kB 7.5MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████████▉      | 901kB 7.5MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████████▏     | 911kB 7.5MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████████▌     | 921kB 7.5MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████████▊     | 931kB 7.5MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████████     | 942kB 7.5MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████████▎    | 952kB 7.5MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████████▋    | 962kB 7.5MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████████    | 972kB 7.5MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████████▏   | 983kB 7.5MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████████▌   | 993kB 7.5MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████████▉   | 1.0MB 7.5MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████████████   | 1.0MB 7.5MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████████████▍  | 1.0MB 7.5MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████████████▊  | 1.0MB 7.5MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████████████  | 1.0MB 7.5MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████████████▎ | 1.1MB 7.5MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████████████▋ | 1.1MB 7.5MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████████████▉ | 1.1MB 7.5MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████████████▏| 1.1MB 7.5MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████████████▍| 1.1MB 7.5MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████████████▊| 1.1MB 7.5MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████████████| 1.1MB 7.5MB/s \n",
      "\u001b[?25hInstalling collected packages: sentencepiece\n",
      "Successfully installed sentencepiece-0.1.94\n",
      "--2020-12-08 19:27:19--  https://raw.githubusercontent.com/moussaKam/transfer_learning_transformers/main/sentencepiece.french.model\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 1115362 (1.1M) [application/octet-stream]\n",
      "Saving to: ‘sentencepiece.french.model’\n",
      "\n",
      "sentencepiece.frenc 100%[===================>]   1.06M  --.-KB/s    in 0.04s   \n",
      "\n",
      "2020-12-08 19:27:19 (25.2 MB/s) - ‘sentencepiece.french.model’ saved [1115362/1115362]\n",
      "\n",
      "['▁Bonjour', '▁les', '▁amis', '!']\n",
      "Bonjour les amis!\n"
     ]
    }
   ],
   "source": [
    "!pip install sentencepiece\n",
    "!wget https://raw.githubusercontent.com/moussaKam/transfer_learning_transformers/main/sentencepiece.french.model\n",
    "\n",
    "import sentencepiece as spm\n",
    "\n",
    "s = spm.SentencePieceProcessor(model_file='sentencepiece.french.model') #load sentencepiece model\n",
    "\n",
    "#examples\n",
    "encoded = s.encode_as_pieces(\"Bonjour les amis!\")\n",
    "decoded = s.decode_pieces(encoded)\n",
    "print(encoded)\n",
    "print(decoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "TtLlV05pkQI3"
   },
   "outputs": [],
   "source": [
    "def infer_next_token(sent):\n",
    "    model.eval()\n",
    "    sent_pieces = s.encode_as_pieces(sent)\n",
    "    source = [token2ind['<sos>']] + [token2ind[el] for el in sent_pieces]\n",
    "    source = torch.tensor(source).to(device)\n",
    "    source = source.reshape(-1, 1)\n",
    "    src_mask = model.base.generate_square_subsequent_mask(source.size(0)).to(device)\n",
    "    out = model(source, src_mask)\n",
    "    next_token_ind = out[-1].argmax(-1)\n",
    "    return next_token_ind, out\n",
    "    \n",
    "def infer_next_tokens(sent, max_len=50):\n",
    "    token=''\n",
    "    i=0\n",
    "    while (token!='<eos>') and (i<50):\n",
    "      i=i+1\n",
    "      token= ind2token[int(infer_next_token(sent)[0])]\n",
    "      if token!='<eos>' :\n",
    "        sent=sent+' '+token[1:len(token)]\n",
    "      else:\n",
    "        sent=sent+'.'\n",
    "\n",
    "    return sent\n",
    "    # to be implemented"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "f83Nn5nSly4v",
    "outputId": "ffaace2d-ce03-4c5d-b6a1-62e15b042642"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bonjour les gens qui ont été très accueillants et sympathiques                                          \n"
     ]
    }
   ],
   "source": [
    "sent = \"Bonjour les\"\n",
    "print(infer_next_tokens(sent))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Lp7mjVzomoZ3"
   },
   "source": [
    "### Supervised task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0K1BZsblmEmx",
    "outputId": "8beab547-e27b-437e-851e-746ae4dfeeeb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2020-12-08 19:27:19--  https://raw.githubusercontent.com/moussaKam/transfer_learning_transformers/main/cls-books/train.review.spm\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 1495960 (1.4M) [text/plain]\n",
      "Saving to: ‘train.review.spm’\n",
      "\n",
      "train.review.spm    100%[===================>]   1.43M  --.-KB/s    in 0.06s   \n",
      "\n",
      "2020-12-08 19:27:20 (25.1 MB/s) - ‘train.review.spm’ saved [1495960/1495960]\n",
      "\n",
      "--2020-12-08 19:27:20--  https://raw.githubusercontent.com/moussaKam/transfer_learning_transformers/main/cls-books/train.label\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 3200 (3.1K) [text/plain]\n",
      "Saving to: ‘train.label’\n",
      "\n",
      "train.label         100%[===================>]   3.12K  --.-KB/s    in 0s      \n",
      "\n",
      "2020-12-08 19:27:20 (40.9 MB/s) - ‘train.label’ saved [3200/3200]\n",
      "\n",
      "--2020-12-08 19:27:20--  https://raw.githubusercontent.com/moussaKam/transfer_learning_transformers/main/cls-books/test.review.spm\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 1864544 (1.8M) [text/plain]\n",
      "Saving to: ‘test.review.spm’\n",
      "\n",
      "test.review.spm     100%[===================>]   1.78M  --.-KB/s    in 0.06s   \n",
      "\n",
      "2020-12-08 19:27:20 (27.4 MB/s) - ‘test.review.spm’ saved [1864544/1864544]\n",
      "\n",
      "--2020-12-08 19:27:20--  https://raw.githubusercontent.com/moussaKam/transfer_learning_transformers/main/cls-books/test.label\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 4000 (3.9K) [text/plain]\n",
      "Saving to: ‘test.label’\n",
      "\n",
      "test.label          100%[===================>]   3.91K  --.-KB/s    in 0s      \n",
      "\n",
      "2020-12-08 19:27:21 (83.7 MB/s) - ‘test.label’ saved [4000/4000]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://raw.githubusercontent.com/moussaKam/transfer_learning_transformers/main/cls-books/train.review.spm\n",
    "!wget https://raw.githubusercontent.com/moussaKam/transfer_learning_transformers/main/cls-books/train.label\n",
    "!wget https://raw.githubusercontent.com/moussaKam/transfer_learning_transformers/main/cls-books/test.review.spm\n",
    "!wget https://raw.githubusercontent.com/moussaKam/transfer_learning_transformers/main/cls-books/test.label\n",
    "\n",
    "path_data_train = \"train.review.spm\"\n",
    "path_labels_train = \"train.label\"\n",
    "\n",
    "path_data_valid = \"test.review.spm\"\n",
    "path_labels_valid = \"test.label\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "_MLfvjiom2SL"
   },
   "outputs": [],
   "source": [
    "# a function to evaluate the validation accuracy of the model.\n",
    "def evaluate_accuracy(data_loader):\n",
    "  acc=0\n",
    "  k=0\n",
    "  for d in data_loader:\n",
    "    data=d[0].to(device)\n",
    "    target=d[1]\n",
    "    \n",
    "    target = target.to(device)\n",
    "    src_mask = model.base.generate_square_subsequent_mask(data.size(0)).to(\n",
    "            device\n",
    "        )\n",
    "    output=model(data, src_mask)\n",
    "    output = output[-1,:,:]\n",
    "    pred=torch.argmax(output,dim=1)\n",
    "    for i in range(len(pred)):\n",
    "      k=k+1\n",
    "      if (pred[i]==target[i]):\n",
    "        acc=acc+1\n",
    "  acc=acc/k\n",
    "  print(\"Accuracy = \",acc)\n",
    "  return acc\n",
    "\n",
    "    #to be implemented"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "qzmx7T7xoa6v"
   },
   "outputs": [],
   "source": [
    "#save the base model to be loaded later in the fine-tuning phase\n",
    "torch.save({\"model_state_dict\": model.base.state_dict(),}, \"pretrained_model_4layers_no_class_head.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "i-xclMCpnVpw",
    "outputId": "1e0db230-677a-43fe-b002-da86bdcb24e6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=====Trainig FROM SCRATCH======\n",
      "| epoch   1 |    50/  200 steps | loss 0.74402 | ppl    2.104\n",
      "| epoch   1 |   100/  200 steps | loss 0.72758 | ppl    2.070\n",
      "| epoch   1 |   150/  200 steps | loss 0.73251 | ppl    2.080\n",
      "Accuracy =  0.5525\n",
      "| epoch   2 |    50/  200 steps | loss 0.63547 | ppl    1.888\n",
      "| epoch   2 |   100/  200 steps | loss 0.60439 | ppl    1.830\n",
      "| epoch   2 |   150/  200 steps | loss 0.71441 | ppl    2.043\n",
      "Accuracy =  0.7135\n",
      "| epoch   3 |    50/  200 steps | loss 0.44744 | ppl    1.564\n",
      "| epoch   3 |   100/  200 steps | loss 0.42450 | ppl    1.529\n",
      "| epoch   3 |   150/  200 steps | loss 0.31913 | ppl    1.376\n",
      "Accuracy =  0.721\n",
      "| epoch   4 |    50/  200 steps | loss 0.17728 | ppl    1.194\n",
      "| epoch   4 |   100/  200 steps | loss 0.11718 | ppl    1.124\n",
      "| epoch   4 |   150/  200 steps | loss 0.19863 | ppl    1.220\n",
      "Accuracy =  0.7565\n",
      "| epoch   5 |    50/  200 steps | loss 0.01171 | ppl    1.012\n",
      "| epoch   5 |   100/  200 steps | loss 0.06223 | ppl    1.064\n",
      "| epoch   5 |   150/  200 steps | loss 0.02048 | ppl    1.021\n",
      "Accuracy =  0.737\n",
      "| epoch   6 |    50/  200 steps | loss 0.00517 | ppl    1.005\n",
      "| epoch   6 |   100/  200 steps | loss 0.01089 | ppl    1.011\n",
      "| epoch   6 |   150/  200 steps | loss 0.01020 | ppl    1.010\n",
      "Accuracy =  0.759\n",
      "| epoch   7 |    50/  200 steps | loss 0.00038 | ppl    1.000\n",
      "| epoch   7 |   100/  200 steps | loss 0.01563 | ppl    1.016\n",
      "| epoch   7 |   150/  200 steps | loss 0.00888 | ppl    1.009\n",
      "Accuracy =  0.7585\n",
      "| epoch   8 |    50/  200 steps | loss 0.00062 | ppl    1.001\n",
      "| epoch   8 |   100/  200 steps | loss 0.00002 | ppl    1.000\n",
      "| epoch   8 |   150/  200 steps | loss 0.00001 | ppl    1.000\n",
      "Accuracy =  0.76\n",
      "| epoch   9 |    50/  200 steps | loss 0.00001 | ppl    1.000\n",
      "| epoch   9 |   100/  200 steps | loss 0.00001 | ppl    1.000\n",
      "| epoch   9 |   150/  200 steps | loss 0.00001 | ppl    1.000\n",
      "Accuracy =  0.7605\n",
      "| epoch  10 |    50/  200 steps | loss 0.00001 | ppl    1.000\n",
      "| epoch  10 |   100/  200 steps | loss 0.00001 | ppl    1.000\n",
      "| epoch  10 |   150/  200 steps | loss 0.00004 | ppl    1.000\n",
      "Accuracy =  0.756\n",
      "| epoch  11 |    50/  200 steps | loss 0.00001 | ppl    1.000\n",
      "| epoch  11 |   100/  200 steps | loss 0.00000 | ppl    1.000\n",
      "| epoch  11 |   150/  200 steps | loss 0.00000 | ppl    1.000\n",
      "Accuracy =  0.7605\n",
      "| epoch  12 |    50/  200 steps | loss 0.00000 | ppl    1.000\n",
      "| epoch  12 |   100/  200 steps | loss 0.00000 | ppl    1.000\n",
      "| epoch  12 |   150/  200 steps | loss 0.00000 | ppl    1.000\n",
      "Accuracy =  0.762\n",
      "| epoch  13 |    50/  200 steps | loss 0.00000 | ppl    1.000\n",
      "| epoch  13 |   100/  200 steps | loss 0.00000 | ppl    1.000\n",
      "| epoch  13 |   150/  200 steps | loss 0.00000 | ppl    1.000\n",
      "Accuracy =  0.7595\n",
      "| epoch  14 |    50/  200 steps | loss 0.00000 | ppl    1.000\n",
      "| epoch  14 |   100/  200 steps | loss 0.00000 | ppl    1.000\n",
      "| epoch  14 |   150/  200 steps | loss 0.00000 | ppl    1.000\n",
      "Accuracy =  0.758\n",
      "| epoch  15 |    50/  200 steps | loss 0.00000 | ppl    1.000\n",
      "| epoch  15 |   100/  200 steps | loss 0.00000 | ppl    1.000\n",
      "| epoch  15 |   150/  200 steps | loss 0.00000 | ppl    1.000\n",
      "Accuracy =  0.76\n",
      "\n",
      "=====PRETRAINED MODEL======\n",
      "| epoch   1 |    50/  200 steps | loss 0.81593 | ppl    2.261\n",
      "| epoch   1 |   100/  200 steps | loss 0.69325 | ppl    2.000\n",
      "| epoch   1 |   150/  200 steps | loss 0.62464 | ppl    1.868\n",
      "Accuracy =  0.718\n",
      "| epoch   2 |    50/  200 steps | loss 0.51601 | ppl    1.675\n",
      "| epoch   2 |   100/  200 steps | loss 0.44554 | ppl    1.561\n",
      "| epoch   2 |   150/  200 steps | loss 0.47141 | ppl    1.602\n",
      "Accuracy =  0.7685\n",
      "| epoch   3 |    50/  200 steps | loss 0.35062 | ppl    1.420\n",
      "| epoch   3 |   100/  200 steps | loss 0.39996 | ppl    1.492\n",
      "| epoch   3 |   150/  200 steps | loss 0.38965 | ppl    1.476\n",
      "Accuracy =  0.8005\n",
      "| epoch   4 |    50/  200 steps | loss 0.26478 | ppl    1.303\n",
      "| epoch   4 |   100/  200 steps | loss 0.27655 | ppl    1.319\n",
      "| epoch   4 |   150/  200 steps | loss 0.27843 | ppl    1.321\n",
      "Accuracy =  0.775\n",
      "| epoch   5 |    50/  200 steps | loss 0.18958 | ppl    1.209\n",
      "| epoch   5 |   100/  200 steps | loss 0.18730 | ppl    1.206\n",
      "| epoch   5 |   150/  200 steps | loss 0.19720 | ppl    1.218\n",
      "Accuracy =  0.7905\n",
      "| epoch   6 |    50/  200 steps | loss 0.12117 | ppl    1.129\n",
      "| epoch   6 |   100/  200 steps | loss 0.19214 | ppl    1.212\n",
      "| epoch   6 |   150/  200 steps | loss 0.16935 | ppl    1.185\n",
      "Accuracy =  0.786\n",
      "| epoch   7 |    50/  200 steps | loss 0.03759 | ppl    1.038\n",
      "| epoch   7 |   100/  200 steps | loss 0.02568 | ppl    1.026\n",
      "| epoch   7 |   150/  200 steps | loss 0.04636 | ppl    1.047\n",
      "Accuracy =  0.7635\n",
      "| epoch   8 |    50/  200 steps | loss 0.00315 | ppl    1.003\n",
      "| epoch   8 |   100/  200 steps | loss 0.01394 | ppl    1.014\n",
      "| epoch   8 |   150/  200 steps | loss 0.01900 | ppl    1.019\n",
      "Accuracy =  0.7845\n",
      "| epoch   9 |    50/  200 steps | loss 0.01362 | ppl    1.014\n",
      "| epoch   9 |   100/  200 steps | loss 0.00086 | ppl    1.001\n",
      "| epoch   9 |   150/  200 steps | loss 0.01188 | ppl    1.012\n",
      "Accuracy =  0.795\n",
      "| epoch  10 |    50/  200 steps | loss 0.01708 | ppl    1.017\n",
      "| epoch  10 |   100/  200 steps | loss 0.00129 | ppl    1.001\n",
      "| epoch  10 |   150/  200 steps | loss 0.00016 | ppl    1.000\n",
      "Accuracy =  0.7875\n",
      "| epoch  11 |    50/  200 steps | loss 0.00001 | ppl    1.000\n",
      "| epoch  11 |   100/  200 steps | loss 0.00001 | ppl    1.000\n",
      "| epoch  11 |   150/  200 steps | loss 0.00033 | ppl    1.000\n",
      "Accuracy =  0.7905\n",
      "| epoch  12 |    50/  200 steps | loss 0.00003 | ppl    1.000\n",
      "| epoch  12 |   100/  200 steps | loss 0.00002 | ppl    1.000\n",
      "| epoch  12 |   150/  200 steps | loss 0.00129 | ppl    1.001\n",
      "Accuracy =  0.7905\n",
      "| epoch  13 |    50/  200 steps | loss 0.00000 | ppl    1.000\n",
      "| epoch  13 |   100/  200 steps | loss 0.00032 | ppl    1.000\n",
      "| epoch  13 |   150/  200 steps | loss 0.00209 | ppl    1.002\n",
      "Accuracy =  0.786\n",
      "| epoch  14 |    50/  200 steps | loss 0.00210 | ppl    1.002\n",
      "| epoch  14 |   100/  200 steps | loss 0.00231 | ppl    1.002\n",
      "| epoch  14 |   150/  200 steps | loss 0.00001 | ppl    1.000\n",
      "Accuracy =  0.787\n",
      "| epoch  15 |    50/  200 steps | loss 0.00005 | ppl    1.000\n",
      "| epoch  15 |   100/  200 steps | loss 0.01253 | ppl    1.013\n",
      "| epoch  15 |   150/  200 steps | loss 0.00429 | ppl    1.004\n",
      "Accuracy =  0.78\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from_scratch_settings = [True, False]\n",
    "\n",
    "from_scratch_valid_acc = []\n",
    "pretrained_valid_acc = []\n",
    "lr = 0.0001\n",
    "\n",
    "for from_scratch in from_scratch_settings:\n",
    "    model = Model(ntokens, nhead, nhid, nlayers, 2, dropout).to(device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    if not from_scratch:\n",
    "        print(\"=====PRETRAINED MODEL======\")\n",
    "        #load checkpoint\n",
    "        checkpoint = torch.load(\"pretrained_model_4layers_no_class_head.pt\")\n",
    "        #load state dict\n",
    "        model.base.load_state_dict(checkpoint['model_state_dict'])\n",
    "    else:\n",
    "        print(\"=====Trainig FROM SCRATCH======\")\n",
    "    epochs = 15\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        train(\n",
    "            path_data_train,\n",
    "            path_labels_train,\n",
    "            save_interval=-1,\n",
    "            task='classification',\n",
    "            batch_size=8,\n",
    "            log_interval=50,\n",
    "        )\n",
    "        acc = evaluate_accuracy(\n",
    "            get_loader(\n",
    "                path_data_valid,\n",
    "                path_labels_valid,\n",
    "                token2ind=token2ind,\n",
    "                batch_size=20,\n",
    "                task='classification',\n",
    "            )\n",
    "        )\n",
    "        if from_scratch:\n",
    "            from_scratch_valid_acc.append(acc)\n",
    "        else:\n",
    "            pretrained_valid_acc.append(acc)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 265
    },
    "id": "RCpBIdTHojm6",
    "outputId": "135a10b0-27f7-4c1a-bbf6-f3b741a7a53b"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light",
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Visualize the accuracy\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot([i for i in range(1,16)],from_scratch_valid_acc,label=\"from scratch\")\n",
    "plt.plot([i for i in range(1,16)],pretrained_valid_acc,label=\"pretrained\")\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left', borderaxespad=0.)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "Lp7mjVzomoZ3"
   ],
   "name": "ALTEGRAD_2020_transfer_learning.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
